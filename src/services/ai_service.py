from typing import List, Dict, Optional, Union
import json
import time
import random
import threading
from pathlib import Path
from dataclasses import dataclass
from datetime import datetime
import asyncio
import concurrent.futures
from google import genai
from google.genai import types


@dataclass
class ConversationMessage:
    """‰ºöË©±„É°„ÉÉ„Çª„Éº„Ç∏"""
    role: str  # "user" or "model"
    content: str
    timestamp: datetime


@dataclass
class TaskInfo:
    """„Çø„Çπ„ÇØÊÉÖÂ†±"""
    title: str
    task_type: Optional[str] = None
    urgency: Optional[str] = None
    due_date: Optional[str] = None
    current_description: Optional[str] = None


@dataclass
class AIAnalysisResult:
    """AIÂàÜÊûêÁµêÊûú"""
    status: str  # "insufficient_info" or "ready_to_format"
    message: str
    suggestions: Optional[List[str]] = None
    formatted_content: Optional[str] = None


class ConversationHistory:
    """‰ºöË©±Â±•Ê≠¥ÁÆ°ÁêÜ"""

    def __init__(self, storage_path: Optional[Union[str, Path]] = None):
        self.lock = threading.Lock()
        self.storage_path = Path(storage_path) if storage_path else Path(".ai_conversations.json")
        self.conversations: Dict[str, List[ConversationMessage]] = {}
        self._load_from_disk()

    def _load_from_disk(self):
        try:
            if self.storage_path.exists():
                data = json.loads(self.storage_path.read_text(encoding="utf-8"))
                for sid, msgs in data.items():
                    self.conversations[sid] = [
                        ConversationMessage(
                            role=m.get("role", "user"),
                            content=m.get("content", ""),
                            timestamp=datetime.fromisoformat(m.get("timestamp"))
                            if m.get("timestamp")
                            else datetime.now(),
                        )
                        for m in msgs
                    ]
        except Exception:
            # Ë™≠„ÅøËæº„ÅøÂ§±ÊïóÊôÇ„ÅØÁ©∫„Å®„Åó„Å¶Êâ±„ÅÜÔºàÂ£ä„Çå„Åü„Éï„Ç°„Ç§„É´„Åß„ÇÇÁ®ºÂÉç„ÇíÊ≠¢„ÇÅ„Å™„ÅÑÔºâ
            self.conversations = {}

    def _flush_to_disk(self):
        try:
            payload = {
                sid: [
                    {
                        "role": m.role,
                        "content": m.content,
                        "timestamp": m.timestamp.isoformat(),
                    }
                    for m in msgs
                ]
                for sid, msgs in self.conversations.items()
            }
            tmp_path = self.storage_path.with_suffix(self.storage_path.suffix + ".tmp")
            tmp_path.write_text(json.dumps(payload, ensure_ascii=False), encoding="utf-8")
            tmp_path.replace(self.storage_path)
        except Exception:
            # Êõ∏„ÅçËæº„ÅøÂ§±Êïó„ÅØËá¥ÂëΩÁöÑ„Åß„ÅØ„Å™„ÅÑ„Åü„ÇÅÊè°„Çä„Å§„Å∂„ÅôÔºà„É≠„Ç∞„ÅØÊ®ôÊ∫ñÂá∫ÂäõÂÅ¥„Å´‰ªª„Åõ„ÇãÔºâ
            pass


class InMemoryConversationHistory:
    """„É°„É¢„É™ÂÜÖ„ÅÆ„Åø„ÅßÁÆ°ÁêÜ„Åô„Çã‰ºöË©±Â±•Ê≠¥Ôºà‰∏ÄÊôÇÁöÑ„Å™„Çª„ÉÉ„Ç∑„Éß„É≥Áî®Ôºâ"""

    def __init__(self):
        self.lock = threading.Lock()
        self.conversations: Dict[str, List[ConversationMessage]] = {}

    def add_message(self, session_id: str, role: str, content: str):
        """„É°„ÉÉ„Çª„Éº„Ç∏„ÇíËøΩÂä†"""
        with self.lock:
            if session_id not in self.conversations:
                self.conversations[session_id] = []
            message = ConversationMessage(role=role, content=content, timestamp=datetime.now())
            self.conversations[session_id].append(message)
            # „É°„É¢„É™ÂÜÖ„Å™„ÅÆ„Åß„ÄÅ„Éá„Ç£„Çπ„ÇØ„Éï„É©„ÉÉ„Ç∑„É•„ÅØ‰∏çË¶ÅÔºàÁ©∫ÂÆüË£ÖÔºâ

    def get_conversation(self, session_id: str) -> List[ConversationMessage]:
        """‰ºöË©±Â±•Ê≠¥„ÇíÂèñÂæó"""
        with self.lock:
            return list(self.conversations.get(session_id, []))

    def clear_conversation(self, session_id: str):
        """‰ºöË©±Â±•Ê≠¥„Çí„ÇØ„É™„Ç¢"""
        with self.lock:
            if session_id in self.conversations:
                del self.conversations[session_id]

    def start_new_session(self, session_id: str):
        """Êñ∞„Åó„ÅÑ„Çª„ÉÉ„Ç∑„Éß„É≥„ÇíÈñãÂßãÔºàÊó¢Â≠ò„ÅÆ‰ºöË©±„Çí„ÇØ„É™„Ç¢Ôºâ"""
        with self.lock:
            self.conversations[session_id] = []

    def _flush_to_disk(self):
        """„É°„É¢„É™ÂÜÖ„ÇØ„É©„Çπ„Å™„ÅÆ„Åß‰Ωï„ÇÇ„Åó„Å™„ÅÑÔºà‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅÔºâ"""
        pass


class TaskAIService:
    """„Çø„Çπ„ÇØ„Ç≥„É≥„ÉÜ„É≥„ÉÑAIÊã°Âºµ„Çµ„Éº„Éì„Çπ"""

    def __init__(self, api_key: str, timeout_seconds: float = 30.0, model_name: str = "gemini-2.5-flash", history_storage_path: Optional[str] = None):
        self.client = genai.Client(api_key=api_key)
        # „É°„É¢„É™ÂÜÖ„ÅÆ„Åø„Åß‰ºöË©±Â±•Ê≠¥„ÇíÁÆ°ÁêÜÔºà„Éï„Ç©„Éº„É†ÂÖ•ÂäõÊôÇ„ÅÆ„Åø„ÅÆ‰∏ÄÊôÇÁöÑ„Å™‰ΩøÁî®Ôºâ
        self.history = InMemoryConversationHistory()
        self.timeout_seconds = timeout_seconds
        self.model_name = model_name
        self.max_retries = 3
        
        # „Ç∑„Çπ„ÉÜ„É†ÊåáÁ§∫ÔºàÁ∞°ÊΩî„Åã„Å§ÊßãÈÄ†ÂåñÂøúÁ≠î„ÇíÂº∑Âà∂Ôºâ
        self.system_instruction = """„ÅÇ„Å™„Åü„ÅØ„Çø„Çπ„ÇØÁÆ°ÁêÜ„ÅÆË£úÂä©AI„Åß„Åô„ÄÇÊèê‰æõ„Åï„Çå„ÅüÊÉÖÂ†±„Çí„ÇÇ„Å®„Å´„ÄÅÂÆüË°åÂèØËÉΩ„Å™„Çø„Çπ„ÇØÊèêÊ°à„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ

ÂøÖ„ÅöÊ¨°„ÅÆ„É´„Éº„É´„Å´Âæì„Å£„Å¶„Åè„Å†„Åï„ÅÑÔºö
- ËøîÁ≠î„ÅØJSON„ÅÆ„Åø„ÄÇÂâçÂæå„Å´Ë™¨Êòé„ÇÑ„Ç≥„Éº„Éâ„Éñ„É≠„ÉÉ„ÇØ„ÄÅ„Ç≥„É°„É≥„Éà„ÅØ‰ªò‰∏é„Åó„Å™„ÅÑ„ÄÇ
- „Çπ„Ç≠„Éº„Éû„Å´Ê∫ñÊã†Ôºöstatus„ÅØ"insufficient_info"„Åæ„Åü„ÅØ"ready_to_format"„ÄÇ
- insufficient„ÅÆÂ†¥Âêà„ÄÅreason„Å®ÂÖ∑‰ΩìÁöÑ„Å™questionsÈÖçÂàóÔºàÁ∞°ÊΩî„Å™Êó•Êú¨Ë™û„ÅÆË≥™ÂïèÊñáÔºâ„ÇíËøî„Åô„ÄÇ
- ready„ÅÆÂ†¥Âêà„ÄÅsuggestion.description„Å´„Éû„Éº„ÇØ„ÉÄ„Ç¶„É≥ÂΩ¢Âºè„Åß‰ª•‰∏ã„ÅÆÈ†ÜÂ∫è„ÅßË®òËø∞„Åô„ÇãÔºàÂøÖ„ÅöÂêÑ„Çª„ÇØ„Ç∑„Éß„É≥Èñì„Å´ÊîπË°å\\n„ÇíÂÖ•„Çå„ÇãÔºâÔºö
  ## ÁõÆÁöÑ„ÉªËÉåÊôØ\\nÔºàÁõÆÁöÑ„ÇÑËÉåÊôØ„ÇíË®òËø∞Ôºâ\\n\\n## ‰ΩúÊ•≠ÂÜÖÂÆπ\\n1. ÔºàÂÖ∑‰ΩìÁöÑ„Å™ÊâãÈ†Ü1Ôºâ\\n2. ÔºàÂÖ∑‰ΩìÁöÑ„Å™ÊâãÈ†Ü2Ôºâ\\n\\n## ÂÆå‰∫ÜÊù°‰ª∂\\nÔºàÂÆå‰∫Ü„ÅÆÂà§Êñ≠Âü∫Ê∫ñÔºâ\\n\\n## Ê≥®ÊÑèÁÇπ\\nÔºàÈáçË¶Å„Å™Ê≥®ÊÑè‰∫ãÈ†ÖÔºâ
  ÂèØËÉΩ„Å™„Çâtitle, category, urgency, due_date_iso„ÇÇË£úÂÆå„Åô„ÇãÔºà‰∏çÊòé„Å™„ÇâÁúÅÁï•ÂèØÔºâ„ÄÇ

ÂàÜÈ°û„ÅÆÊåáÈáùÔºàÂèÇËÄÉÔºâÔºö
- Á§æÂÜÖ„Çø„Çπ„ÇØ / ÊäÄË°ìË™øÊüª / È°ßÂÆ¢ÂØæÂøú / Âñ∂Ê•≠ÈÄ£Áµ° / Ë¶Å‰ª∂ÂÆöÁæ© / Ë≥áÊñô‰ΩúÊàê / „Åù„ÅÆ‰ªñ

Á∞°ÊΩî„Åß„ÄÅ„Åô„ÅêÂÆüË°åÂèØËÉΩ„Å™ÂΩ¢„Å´Êï¥„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""

        # „Éá„Éï„Ç©„É´„Éà„ÅÆ„É¢„Éá„É´ÂêçÔºà‰∏ä‰Ωç„Åã„ÇâÊ≥®ÂÖ•„Åï„Çå„ÇãÊÉ≥ÂÆöÔºâ
        if not hasattr(self, "model_name"):
            self.model_name = "gemini-2.5-flash"
    
    def _response_schema(self) -> types.Schema:
        """Gemini„ÅÆÊßãÈÄ†ÂåñÂá∫Âäõ„Çπ„Ç≠„Éº„Éû„ÇíÂÆöÁæ©"""
        return types.Schema(
            type=types.Type.OBJECT,
            properties={
                "status": types.Schema(type=types.Type.STRING, enum=["insufficient_info", "ready_to_format"]),
                "reason": types.Schema(type=types.Type.STRING),
                "questions": types.Schema(
                    type=types.Type.ARRAY,
                    items=types.Schema(type=types.Type.STRING),
                ),
                "suggestion": types.Schema(
                    type=types.Type.OBJECT,
                    properties={
                        "title": types.Schema(type=types.Type.STRING),
                        "category": types.Schema(type=types.Type.STRING),
                        "urgency": types.Schema(type=types.Type.STRING),
                        "due_date_iso": types.Schema(type=types.Type.STRING),
                        "description": types.Schema(type=types.Type.STRING),
                    },
                ),
            },
        )

    def _build_contents(self, session_id: str, user_text: Optional[str] = None) -> List[types.Content]:
        """Â±•Ê≠¥ + Áõ¥Ëøë„É¶„Éº„Ç∂„ÉºÊåáÁ§∫„Åã„ÇâContents„Çí‰Ωú„Çã"""
        contents: List[types.Content] = []
        conversation = self.history.get_conversation(session_id)

        print(f"üîç [_build_contents] „Çª„ÉÉ„Ç∑„Éß„É≥ {session_id}: Â±•Ê≠¥Êï∞={len(conversation)}")

        for i, msg in enumerate(conversation):
            role = "user" if msg.role == "user" else "model"
            print(f"  Â±•Ê≠¥[{i}] {role}: {msg.content[:100]}...")
            contents.append(
                types.Content(role=role, parts=[types.Part.from_text(text=msg.content)])
            )

        if user_text:
            print(f"  Êñ∞Ë¶è„É¶„Éº„Ç∂„ÉºÂÖ•Âäõ: {user_text[:100]}...")
            contents.append(types.Content(role="user", parts=[types.Part.from_text(text=user_text)]))

        print(f"üîç [_build_contents] ÊúÄÁµÇÁöÑ„Å™contentsÊï∞: {len(contents)}")
        return contents

    def _call_ai_with_timeout(self, contents: Union[str, List[types.Content]], timeout: Optional[float] = None) -> str:
        """„Çø„Ç§„É†„Ç¢„Ç¶„Éà + „É™„Éà„É©„Ç§‰ªò„Åç„ÅßAI„ÇíÂëº„Å≥Âá∫„Åô"""
        effective_timeout = timeout or self.timeout_seconds
        def call_ai():
            attempts = self.max_retries
            last_err: Optional[Exception] = None
            for i in range(attempts):
                try:
                    response = self.client.models.generate_content(
                        model=self.model_name,
                        contents=contents,
                        config=types.GenerateContentConfig(
                            thinking_config=types.ThinkingConfig(thinking_budget=0),
                            max_output_tokens=1000,
                            temperature=0.2,
                            system_instruction=self.system_instruction,
                            response_mime_type="application/json",
                            response_schema=self._response_schema(),
                        ),
                    )
                    return response.text
                except Exception as e:
                    msg = str(e).lower()
                    retryable = any(k in msg for k in [
                        "unavailable", "overloaded", "please try again", "deadline", "temporarily", "resource exhausted", "rate"
                    ])
                    last_err = e
                    if retryable and i < attempts - 1:
                        # ÊåáÊï∞„Éê„ÉÉ„ÇØ„Ç™„Éï + „Ç∏„ÉÉ„Çø
                        sleep_s = (0.6 * (2 ** i)) + random.uniform(0, 0.3)
                        time.sleep(sleep_s)
                        continue
                    raise

        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(call_ai)
            try:
                return future.result(timeout=effective_timeout)
            except concurrent.futures.TimeoutError:
                raise Exception("AI processing timeout - Âá¶ÁêÜÊôÇÈñì„ÅåÈï∑„Åô„Åé„Åæ„Åô")
    
    async def _call_ai_with_timeout_async(self, contents: Union[str, List[types.Content]], timeout: Optional[float] = None) -> str:
        """ÈùûÂêåÊúü„Åß„Çø„Ç§„É†„Ç¢„Ç¶„Éà + „É™„Éà„É©„Ç§‰ªò„Åç„ÅßAI„ÇíÂëº„Å≥Âá∫„Åô"""
        import asyncio
        effective_timeout = timeout or self.timeout_seconds
        
        def call_ai():
            attempts = self.max_retries
            last_err: Optional[Exception] = None
            for i in range(attempts):
                try:
                    response = self.client.models.generate_content(
                        model=self.model_name,
                        contents=contents,
                        config=types.GenerateContentConfig(
                            thinking_config=types.ThinkingConfig(thinking_budget=0),
                            max_output_tokens=1000,
                            temperature=0.2,
                            system_instruction=self.system_instruction,
                            response_mime_type="application/json",
                            response_schema=self._response_schema(),
                        ),
                    )
                    return response.text
                except Exception as e:
                    last_err = e
                    print(f"‚ùå AI call attempt {i+1}/{attempts} failed: {e}")
                    if i < attempts - 1:
                        time.sleep(2 ** i)  # ÊåáÊï∞„Éê„ÉÉ„ÇØ„Ç™„Éï
            raise last_err or Exception("All attempts failed")
        
        # Âà•„Çπ„É¨„ÉÉ„Éâ„ÅßÂÆüË°å„Åó„Å¶Èùû„Éñ„É≠„ÉÉ„Ç≠„É≥„Ç∞Âåñ
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, call_ai)
        except asyncio.TimeoutError:
            raise Exception("AI processing timeout - Âá¶ÁêÜÊôÇÈñì„ÅåÈï∑„Åô„Åé„Åæ„Åô")
    
    async def analyze_task_info(self, session_id: str, task_info: TaskInfo) -> AIAnalysisResult:
        """„Çø„Çπ„ÇØÊÉÖÂ†±„ÇíÂàÜÊûê"""
        try:
            print(f"ü§ñ AIÂàÜÊûêÈñãÂßã: session_id={session_id}")
            # ÁèæÂú®„ÅÆ„Çø„Çπ„ÇØÊÉÖÂ†±„Çí„Éó„É≠„É≥„Éó„Éà„Å´Êï¥ÁêÜ
            prompt = self._build_analysis_prompt(task_info)
            print(f"üîç „Éó„É≠„É≥„Éó„Éà‰ΩúÊàêÂÆå‰∫Ü: {len(prompt)}ÊñáÂ≠ó")
            # Â±•Ê≠¥„Å´„É¶„Éº„Ç∂„ÉºÁô∫Ë©±„ÇíËøΩÂä†„Åó„ÄÅÂ±•Ê≠¥Ëæº„Åø„ÅÆcontents„ÇíÊßãÁØâ
            self.history.add_message(session_id, "user", prompt)
            contents = self._build_contents(session_id)
            print(f"üîç „Ç≥„É≥„ÉÜ„É≥„ÉÑÊßãÁØâÂÆå‰∫Ü: {len(str(contents))}ÊñáÂ≠ó")
            # „Çø„Ç§„É†„Ç¢„Ç¶„ÉàÔºàË®≠ÂÆöÂÄ§Ôºâ‰ªò„Åç„ÅßGemini API„Å´ÈÄÅ‰ø°ÔºàÊßãÈÄ†ÂåñJSON„ÇíÊúüÂæÖÔºâ
            print("üîç Gemini APIÂëº„Å≥Âá∫„ÅóÈñãÂßã...")
            response_text = await self._call_ai_with_timeout_async(contents)
            print(f"‚úÖ Gemini APIÂëº„Å≥Âá∫„ÅóÂÆå‰∫Ü: {len(response_text)}ÊñáÂ≠ó")
            
            # „É¨„Çπ„Éù„É≥„Çπ„Çí‰ºöË©±Â±•Ê≠¥„Å´ËøΩÂä†
            self.history.add_message(session_id, "model", response_text)
            
            # „É¨„Çπ„Éù„É≥„Çπ„ÇíËß£Êûê
            print("üîç „É¨„Çπ„Éù„É≥„ÇπËß£Êûê‰∏≠...")
            result = self._parse_ai_response(response_text)
            print(f"‚úÖ AIÂàÜÊûêÂÆå‰∫Ü: status={result.status}")
            return result
            
        except Exception as e:
            print(f"‚ùå AI analysis error: {e}")
            return AIAnalysisResult(
                status="error",
                message=f"AIÂàÜÊûê„Åß„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}"
            )
    
    async def refine_content(self, session_id: str, feedback: str) -> AIAnalysisResult:
        """„É¶„Éº„Ç∂„Éº„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„ÇíÂü∫„Å´„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÇíÊîπËâØ"""
        try:
            print(f"üîÑ AIÊîπËâØÈñãÂßã: session_id={session_id}")
            user_turn = f"‰ª•‰∏ã„ÅÆ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„ÇíÂèçÊò†„Åó„Å¶ÊîπÂñÑ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÂøÖË¶Å„Å™„Çâ‰∏çË∂≥ÁÇπ„ÇÇË≥™Âïè„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n{feedback}"
            # Â±•Ê≠¥„Å´„É¶„Éº„Ç∂„ÉºÁô∫Ë©±„ÇíËøΩÂä†„Åó„ÄÅÂ±•Ê≠¥Ëæº„Åø„ÅÆcontents„ÇíÊßãÁØâ
            self.history.add_message(session_id, "user", user_turn)
            contents = self._build_contents(session_id)
            # „Çø„Ç§„É†„Ç¢„Ç¶„ÉàÔºàË®≠ÂÆöÂÄ§Ôºâ‰ªò„Åç„ÅßGemini API„Å´ÈÄÅ‰ø°ÔºàÊßãÈÄ†ÂåñJSON„ÇíÊúüÂæÖÔºâ
            print("üîç Gemini APIÂëº„Å≥Âá∫„ÅóÈñãÂßãÔºàÊîπËâØÔºâ...")
            response_text = await self._call_ai_with_timeout_async(contents)
            print(f"‚úÖ Gemini APIÂëº„Å≥Âá∫„ÅóÂÆå‰∫ÜÔºàÊîπËâØÔºâ: {len(response_text)}ÊñáÂ≠ó")
            
            # „É¨„Çπ„Éù„É≥„Çπ„Çí‰ºöË©±Â±•Ê≠¥„Å´ËøΩÂä†
            self.history.add_message(session_id, "model", response_text)
            
            result = self._parse_ai_response(response_text)
            print(f"‚úÖ AIÊîπËâØÂÆå‰∫Ü: status={result.status}")
            return result
            
        except Exception as e:
            print(f"‚ùå AI refinement error: {e}")
            return AIAnalysisResult(
                status="error", 
                message=f"AIÊîπËâØ„Åß„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}"
            )
    
    def clear_session(self, session_id: str):
        """„Çª„ÉÉ„Ç∑„Éß„É≥„Çí„ÇØ„É™„Ç¢"""
        self.history.clear_conversation(session_id)
    
    def _build_analysis_prompt(self, task_info: TaskInfo) -> str:
        """ÂàÜÊûêÁî®„Éó„É≠„É≥„Éó„Éà„ÇíÊßãÁØâ"""
        prompt_parts = [
            "‰ª•‰∏ã„ÅÆ„Çø„Çπ„ÇØÊÉÖÂ†±„ÇíÂàÜÊûê„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö",
            "",
            f"„Çø„Ç§„Éà„É´: {task_info.title}"
        ]
        
        if task_info.task_type:
            prompt_parts.append(f"„Çø„Çπ„ÇØÁ®ÆÈ°û: {task_info.task_type}")
        if task_info.urgency:
            prompt_parts.append(f"Á∑äÊÄ•Â∫¶: {task_info.urgency}")
        if task_info.due_date:
            prompt_parts.append(f"Á¥çÊúü: {task_info.due_date}")
        if task_info.current_description:
            prompt_parts.append(f"ÁèæÂú®„ÅÆÂÜÖÂÆπ: {task_info.current_description}")
        
        return "\n".join(prompt_parts)
    
    def _parse_ai_response(self, response_text: str) -> AIAnalysisResult:
        """AI„É¨„Çπ„Éù„É≥„Çπ„ÇíËß£ÊûêÔºàJSONÂÑ™ÂÖà„ÄÅÂ§±ÊïóÊôÇ„ÅØ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ"""
        # 1) „Åæ„ÅöJSON„Å®„Åó„Å¶Ëß£Èáà
        try:
            data = json.loads(response_text)
            status = data.get("status")

            if status == "insufficient_info":
                reason = data.get("reason") or "ËøΩÂä†ÊÉÖÂ†±„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ"
                questions = data.get("questions") or []
                # ÊñáÂ≠óÂàó„ÅßÊù•„Çã„Åì„Å®„ÇÇËÄÉÊÖÆ
                if isinstance(questions, str):
                    questions = [questions]
                return AIAnalysisResult(
                    status="insufficient_info",
                    message=reason,
                    suggestions=questions,
                )

            if status in ("ready_to_format", "ready"):
                suggestion = data.get("suggestion") or {}
                desc = suggestion.get("description")
                if not desc:
                    # ÊúÄ‰ΩéÈôê„ÅÆÊï¥ÂΩ¢„ÇíË°å„ÅÜ
                    title = suggestion.get("title") or "„Çø„Çπ„ÇØ"
                    category = suggestion.get("category")
                    urgency = suggestion.get("urgency")
                    due = suggestion.get("due_date_iso")
                    meta = []
                    if category:
                        meta.append(f"„Ç´„ÉÜ„Ç¥„É™: {category}")
                    if urgency:
                        meta.append(f"Á∑äÊÄ•Â∫¶: {urgency}")
                    if due:
                        meta.append(f"Á¥çÊúü: {due}")
                    meta_text = ("\n" + " / ".join(meta)) if meta else ""
                    desc = f"„Äê{title}„Äë{meta_text}\n\n## ÁõÆÁöÑ„ÉªËÉåÊôØ\n‰∏çÊòéÁ¢∫„Å™ÁÇπ„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ\n\n## ‰ΩúÊ•≠ÂÜÖÂÆπ\n1. ÂøÖË¶Å„Å™ÊâãÈ†Ü„ÇíÂÆüÊñΩ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n## ÂÆå‰∫ÜÊù°‰ª∂\nÂêàÊÑèÊ∏à„Åø„ÅÆÂèó„ÅëÂÖ•„ÇåÂü∫Ê∫ñ„ÇíÊ∫Ä„Åü„Åô„Åì„Å®„ÄÇ\n\n## Ê≥®ÊÑèÁÇπ\nÈñ¢‰øÇËÄÖ„Å®„ÅÆË™çË≠òÂêà„Çè„Åõ„ÇíË°å„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"

                return AIAnalysisResult(
                    status="ready_to_format",
                    message="ÁîüÊàê„Å´ÊàêÂäü„Åó„Åæ„Åó„Åü",
                    formatted_content=desc.strip(),
                )
        except Exception:
            pass

        # 2) „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºö„Ç≠„Éº„ÉØ„Éº„Éâ„Éô„Éº„Çπ
        try:
            lines = response_text.split("\n")
            if any(keyword in response_text.lower() for keyword in ["‰∏çË∂≥", "Ë∂≥„Çä„Å™„ÅÑ", "ÂøÖË¶Å„Åß„Åô", "Êïô„Åà„Å¶", "„Å©„ÅÆ"]):
                suggestions = []
                for line in lines:
                    if "?" in line or "Ôºü" in line or line.strip().startswith("-"):
                        suggestions.append(line.strip())
                if not suggestions:
                    suggestions = ["ËøΩÂä†„ÅÆÊÉÖÂ†±„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"]
                return AIAnalysisResult(
                    status="insufficient_info",
                    message=response_text,
                    suggestions=suggestions,
                )
            # „Åù„Çå‰ª•Â§ñ„ÅØÂÆåÊàê„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Å®„Åó„Å¶Êâ±„ÅÜ
            return AIAnalysisResult(
                status="ready_to_format",
                message="ÁîüÊàê„Å´ÊàêÂäü„Åó„Åæ„Åó„Åü",
                formatted_content=response_text.strip(),
            )
        except Exception as e:
            print(f"‚ùå Response parsing error: {e}")
            return AIAnalysisResult(status="error", message=f"„É¨„Çπ„Éù„É≥„ÇπËß£Êûê„Ç®„É©„Éº: {str(e)}")
